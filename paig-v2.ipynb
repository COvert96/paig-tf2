{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8030548,"sourceType":"datasetVersion","datasetId":4733304}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-14T14:32:53.849832Z","iopub.status.idle":"2024-04-14T14:32:53.850272Z","shell.execute_reply.started":"2024-04-14T14:32:53.850065Z","shell.execute_reply":"2024-04-14T14:32:53.850083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.851933Z","iopub.status.idle":"2024-04-14T14:32:53.853073Z","shell.execute_reply.started":"2024-04-14T14:32:53.852742Z","shell.execute_reply":"2024-04-14T14:32:53.852777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/utils/math.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.854506Z","iopub.status.idle":"2024-04-14T14:32:53.854937Z","shell.execute_reply.started":"2024-04-14T14:32:53.854744Z","shell.execute_reply":"2024-04-14T14:32:53.854761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/utils/viz.py","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\ndef gallery(array, ncols=3):\n    nindex, height, width, intensity = array.shape\n\n    bordered = 0.5*np.ones([nindex, height+2, width+2, intensity])\n    for i in range(nindex):\n        bordered[i,1:-1,1:-1,:] = array[i]\n\n    array = bordered\n    nindex, height, width, intensity = array.shape\n\n    nrows = nindex//ncols\n    assert nindex == nrows*ncols\n    # want result.shape = (height*nrows, width*ncols, intensity)\n    result = (array.reshape(nrows, ncols, height, width, intensity)\n              .swapaxes(1,2)\n              .reshape(height*nrows, width*ncols, intensity))\n    return result\n\ndef gif(filename, array, fps=10, scale=1.0):\n    from moviepy.editor import ImageSequenceClip\n    \"\"\"Creates a gif given a stack of images using moviepy\n    Notes\n    -----\n    works with current Github version of moviepy (not the pip version)\n    https://github.com/Zulko/moviepy/commit/d4c9c37bc88261d8ed8b5d9b7c317d13b2cdf62e\n    Usage\n    -----\n    >>> X = randn(100, 64, 64)\n    >>> gif('test.gif', X)\n    Parameters\n    ----------\n    filename : string\n        The filename of the gif to write to\n    array : array_like\n        A numpy array that contains a sequence of images\n    fps : int\n        frames per second (default: 10)\n    scale : float\n        how much to rescale each image by (default: 1.0)\n    \"\"\"\n\n    # ensure that the file has the .gif extension\n    fname, _ = os.path.splitext(filename)\n    filename = fname + '.gif'\n\n    # copy into the color dimension if the images are black and white\n    if array.ndim == 3:\n        array = array[..., np.newaxis] * np.ones(3)\n\n    # make the moviepy clip\n    clip = ImageSequenceClip(list(array), fps=fps).resize(scale)\n    clip.write_gif(filename, fps=fps)\n    return clip","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.856172Z","iopub.status.idle":"2024-04-14T14:32:53.856608Z","shell.execute_reply.started":"2024-04-14T14:32:53.856392Z","shell.execute_reply":"2024-04-14T14:32:53.856408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/utils/misc.py","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport inspect\nimport numpy as np\nimport zipfile\n\ndef log_metrics(logger, prefix, metrics):\n    metrics_string = \" \".join([k+\"=%s\"%metrics[k] for k in sorted(metrics.keys())])\n    string = prefix + \" \" + metrics_string\n    logger.info(string)\n\ndef classes_in_module(module):\n    classes = {}\n    for name, obj in inspect.getmembers(module):\n        if inspect.isclass(obj):\n            if obj.__module__ == module.__name__:\n                classes[name] = obj\n    return classes\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n\ndef zipdir(path, save_dir):\n    zipf = zipfile.ZipFile(os.path.join(save_dir, 'code.zip'), 'w', zipfile.ZIP_DEFLATED)\n\n    # ziph is zipfile handle\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if file.split(\".\")[-1] == \"py\":\n                zipf.write(os.path.join(root, file),\n                           os.path.relpath(os.path.join(root, file), os.path.join(path, '..')))\n\n    zipf.close()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.858092Z","iopub.status.idle":"2024-04-14T14:32:53.858488Z","shell.execute_reply.started":"2024-04-14T14:32:53.858300Z","shell.execute_reply":"2024-04-14T14:32:53.858316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/network/base.py","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport shutil\nimport logging\nimport numpy as np\nimport tensorflow as tf\n\nlogger = logging.getLogger(\"tf\")\nroot_path = \"/kaggle/working\"\n\nOPTIMIZERS = {\n    \"adam\": tf.keras.optimizers.Adam,\n    \"rmsprop\": tf.keras.optimizers.RMSprop,\n    \"momentum\": lambda lr: tf.keras.optimizers.SGD(lr, momentum=0.9),\n    \"sgd\": tf.keras.optimizers.SGD\n}\n# OPTIMIZERS = {\n#     \"adam\": tf.compat.v1.train.AdamOptimizer,\n#     \"rmsprop\": tf.compat.v1.train.RMSPropOptimizer,\n#     \"momentum\": lambda x: tf.compat.v1.train.MomentumOptimizer(x, 0.9),\n#     \"sgd\": tf.compat.v1.train.GradientDescentOptimizer\n# }\n\nimport os\nimport sys\nimport shutil\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras  # Import Keras\n\nlogger = logging.getLogger(\"tf\")\nroot_path = \"/kaggle/working\"\n\n\nclass BaseNet(keras.Model):\n    def __init__(self):\n        super(BaseNet, self).__init__()\n\n        self.train_metrics = {}\n        self.eval_metrics = {}\n\n        # Extra functions (consider if these fit better as custom layers)\n        self.extra_train_fns = []\n        self.extra_valid_fns = []\n        self.extra_test_fns = []\n\n    def call(self, inputs):\n        \"\"\"Defines the forward pass, replacing 'feedforward' \"\"\"   \n        raise NotImplementedError  \n\n    def compute_loss(self, inputs, targets): \n        \"\"\"Calculates the loss, replacing the old function\"\"\"\n        raise NotImplementedError \n\n    # ... (Adapted or removed existing methods considering Keras functionality) ...\n\n    def initialize_graph(self, save_dir, use_ckpt, ckpt_dir=\"\"):\n        self.save_dir = save_dir\n        \n        if os.path.exists(save_dir) and not use_ckpt:\n            logger.info(\"Folder exists, deleting...\")\n            shutil.rmtree(save_dir)\n        \n        os.makedirs(save_dir, exist_ok=True)\n\n        if use_ckpt and ckpt_dir:\n            self.load_weights(os.path.join(ckpt_dir, \"model.ckpt\"))  # Adjust as needed\n        elif use_ckpt:\n            self.load_weights(os.path.join(save_dir, \"model.ckpt\"))  # Fallback to save_dir if ckpt_dir not provided\n\n    def save_model(self, epoch=None):\n        \"\"\"Saves the model in the TensorFlow SavedModel format or HDF5 format\"\"\"\n        file_name = \"model\"\n        if epoch is not None:\n            file_name += f\"_epoch_{epoch}\"\n        self.save(os.path.join(self.save_dir, file_name))\n\n    def build_optimizer(self, base_lr, optimizer=\"adam\", anneal_lr=True):\n        self.base_lr = base_lr\n        self.anneal_lr = anneal_lr\n        if optimizer == 'adam':\n            self.optimizer = tf.keras.optimizers.Adam(learning_rate=base_lr)\n        elif optimizer == 'rmsprop':\n            self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_lr)\n        elif optimizer == 'momentum':\n            self.optimizer = lambda lr: tf.keras.optimizers.SGD(lr, momentum=0.9),\n        elif optimizer == 'sgd':\n            self.optimizer = tf.keras.optimizers.SGD\n        # ... (Add other optimizers if needed) ...\n\n    def train_step(self, data):  # Replace 'train' for Keras compatibility\n        x, y = data\n        with tf.GradientTape() as tape:\n            predictions = self(x, training=True)  # Model's forward pass\n            loss = self.compute_loss(x, y) \n\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n        # Update metrics\n        for metric_name, metric_fn in self.train_metrics.items():\n            self.compiled_metrics.update_state(y, predictions) # Assume compiled_metrics exists \n\n        return {m.name: m.result() for m in self.metrics} \n\n    def test_step(self, data):\n        x, y = data\n        with tf.GradientTape() as tape:\n            predictions = self(x, training=True)  # Model's forward pass\n            loss = self.compute_loss(x, y) \n\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n        # Update metrics\n        for metric_name, metric_fn in self.train_metrics.items():\n            self.compiled_metrics.update_state(y, predictions) # Assume compiled_metrics exists \n\n        return {m.name: m.result() for m in self.metrics}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.862783Z","iopub.status.idle":"2024-04-14T14:32:53.863976Z","shell.execute_reply.started":"2024-04-14T14:32:53.863469Z","shell.execute_reply":"2024-04-14T14:32:53.863505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/network/blocks.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\n\"\"\" Useful subnetwork components \"\"\"\n\nimport tensorflow as tf\nfrom tensorflow import keras  \n\n\ndef unet(inp, base_channels, out_channels, upsamp=True):\n    # First block\n    h = keras.layers.Conv2D(base_channels, 3, activation='relu', padding='same')(inp)\n    h1 = keras.layers.Conv2D(base_channels, 3, activation='relu', padding='same')(h)\n\n    # Downsampling blocks\n    blocks = [] # Store downsampling blocks for skip connections\n    for num_filters in [base_channels * 2, base_channels * 4, base_channels * 8]:\n        h = keras.layers.MaxPooling2D(2)(h1)  \n        h = keras.layers.Conv2D(num_filters, 3, activation='relu', padding='same')(h)\n        h = keras.layers.Conv2D(num_filters, 3, activation='relu', padding='same')(h)\n        blocks.append(h)\n\n    # Bottleneck\n    h = keras.layers.Conv2D(base_channels * 8, 3, activation='relu', padding='same')(blocks[-1])\n    h4 = keras.layers.Conv2D(base_channels * 8, 3, activation='relu', padding='same')(h)  \n\n    # Upsampling blocks\n    for i, block in enumerate(reversed(blocks)): \n        if upsamp:\n            h = keras.layers.UpSampling2D()(h4)\n        else:\n            h = keras.layers.Conv2DTranspose(base_channels * 4 // (2 ** i), 3, strides=2, activation='relu', padding='same')(h4)\n\n        h = keras.layers.concatenate([h, block])  # Skip connection\n        num_filters = base_channels * 4 // (2 ** i)  # Reduce filters as we upsample\n        h = keras.layers.Conv2D(num_filters, 3, activation='relu', padding='same')(h)\n        h = keras.layers.Conv2D(num_filters, 3, activation='relu', padding='same')(h)\n\n    # Final output\n    h = keras.layers.Conv2D(out_channels, 1, activation=None, padding='same')(h)\n    return h \n\n\ndef shallow_unet(inp, base_channels, out_channels, upsamp=True):\n    # First block\n    h = keras.layers.Conv2D(base_channels, 3, activation='relu', padding='same')(inp)\n    h1 = keras.layers.Conv2D(base_channels, 3, activation='relu', padding='same')(h)\n\n    # Downsampling\n    h = keras.layers.MaxPooling2D(2)(h1)  \n    h = keras.layers.Conv2D(base_channels * 2, 3, activation='relu', padding='same')(h)\n    h2 = keras.layers.Conv2D(base_channels * 2, 3, activation='relu', padding='same')(h)\n\n    # Upsampling\n    if upsamp:\n        h = keras.layers.UpSampling2D()(h2)\n    else:\n        h = keras.layers.Conv2DTranspose(base_channels * 2, 3, strides=2, activation='relu', padding='same')(h2)\n\n    h = keras.layers.concatenate([h, h2])  # Skip connection\n    h = keras.layers.Conv2D(base_channels * 2, 3, activation='relu', padding='same')(h)\n    h = keras.layers.Conv2D(base_channels * 2, 3, activation='relu', padding='same')(h)\n\n    # Final block\n    if upsamp:\n        h = keras.layers.UpSampling2D()(h)\n    else:\n        h = keras.layers.Conv2DTranspose(base_channels, 3, strides=2, activation='relu', padding='same')(h)\n\n    h = keras.layers.concatenate([h, h1])  # Skip connection\n    h = keras.layers.Conv2D(base_channels, 3, activation='relu', padding='same')(h)\n    h = keras.layers.Conv2D(base_channels, 3, activation='relu', padding='same')(h)\n\n    # Output layer\n    h = keras.layers.Conv2D(out_channels, 1, activation=None, padding='same')(h)\n    return h\n\n    \ndef variable_from_network(shape):\n    var = keras.layers.Dense(200, activation='tanh')(tf.ones([1,10]))\n    var = keras.layers.Dense(np.prod(shape), activation=None)(var)\n    var = tf.reshape(var, shape)\n    return var ","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.866111Z","iopub.status.idle":"2024-04-14T14:32:53.866607Z","shell.execute_reply.started":"2024-04-14T14:32:53.866385Z","shell.execute_reply":"2024-04-14T14:32:53.866404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/network/cells.py","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\n\nclass ODECell(keras.layers.Layer):\n    \"\"\"Base class for ODE cells. Provides common infrastructure.\"\"\"\n\n    def __init__(self, units, **kwargs):\n        super(CustomODECell, self).__init__(**kwargs)\n        self._units = units\n\n    @property\n    def state_size(self):\n        # Assuming state consists of (positions, velocities)\n        return self._units, self._units\n\n    def zero_state(self, batch_size, dtype):\n        positions = tf.zeros([batch_size, self._units], dtype=dtype)\n        velocities = tf.zeros([batch_size, self._units], dtype=dtype)\n        return positions, velocities\n    \nclass BouncingODECell(ODECell):\n    def __init__(self, units, **kwargs):\n        super(BouncingODECell, self).__init__(units, **kwargs)\n        self.dt = self.add_weight(name=\"dt_x\", shape=[], initializer='ones', trainable=False)  # Use weight\n\n    def call(self, inputs, states):\n        positions, velocities = states\n\n        for _ in range(5): \n            positions += self.dt / 5 * velocities\n\n            for j in range(2):\n                # Simplified with NumPy-like broadcasting\n                velocities = tf.where(positions + 2 > 32, -velocities, velocities)\n                velocities = tf.where(positions - 2 < 0, -velocities, velocities)\n                positions = tf.where(positions + 2 > 32, 32 - (positions + 2 - 32) - 2, positions)\n                positions = tf.where(positions - 2 < 0, -(positions - 2) + 2, positions)\n\n        return positions, velocities  # Return new state\n\nclass SpringODECell(ODECell):\n    def __init__(self, units, **kwargs):\n        super(SpringODECell, self).__init__(units, **kwargs)\n        self.dt = self.add_weight(name=\"dt_x\", shape=[], initializer='ones', trainable=False)\n        self.k = self.add_weight(name=\"log_k\", shape=[], trainable=True)  \n        self.equil = self.add_weight(name=\"log_l\", shape=[],  trainable=True) \n\n    def call(self, inputs, states):\n        positions, velocities = states\n\n        for _ in range(5):\n            norm = tf.sqrt(tf.reduce_sum(tf.square(positions[:, :2] - positions[:, 2:]), axis=-1, keepdims=True))\n            direction = (positions[:, :2] - positions[:, 2:]) / (norm + 1e-4)\n            F = tf.exp(self.k) * (norm - 2 * tf.exp(self.equil)) * direction \n\n            velocities[:, :2] -= self.dt / 5 * F\n            velocities[:, 2:] += self.dt / 5 * F  \n\n            positions = positions + self.dt/5 * velocities \n\n        return positions, velocities \n\nclass GravityODECell(ODECell):\n    def __init__(self, units, **kwargs):\n        super(GravityODECell, self).__init__(units, **kwargs)\n        self.dt = self.add_weight(name=\"dt_x\", shape=[], initializer='ones', trainable=False)\n        self.g = self.add_weight(name=\"log_g\", shape=[], trainable=True)\n        self.m = self.add_weight(name=\"log_m\", shape=[], trainable=False)  \n        self.A = tf.exp(self.g) * tf.exp(2 * self.m)\n\n    def call(self, inputs, states):\n        positions, velocities = states\n\n        for _ in range(5):\n            # Compute relative vectors\n            rel_vecs = [positions[:, i:i+2] - positions[:, j:j+2] \n                        for i, j in [(0, 2), (2, 4), (4, 0)]] \n\n            # Compute norms\n            norms = [tf.sqrt(tf.reduce_sum(tf.square(vec), axis=-1, keepdims=True)) for vec in rel_vecs]\n\n            # Calculate forces (with clipping for numerical stability)\n            forces = []\n            for vec, norm in zip(rel_vecs, norms):\n                f = vec / tf.pow(tf.clip_by_value(norm, 1, 170), 3)  \n                forces.append(f)  \n\n            forces = [forces[0] - forces[2], forces[1] - forces[0], forces[2] - forces[1]]  # Net force on each object\n            forces = [-self.A * f for f in forces] \n\n            # Update velocities and positions\n            velocities += self.dt / 5 * tf.concat(forces, axis=1)\n            positions += self.dt / 5 * velocities\n\n        return positions, velocities ","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.867968Z","iopub.status.idle":"2024-04-14T14:32:53.869269Z","shell.execute_reply.started":"2024-04-14T14:32:53.868926Z","shell.execute_reply":"2024-04-14T14:32:53.868960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/network/stn.py","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef _interpolate(im, x, y, out_size):\n    \"\"\"Bilinear interpolation using TensorFlow operations.\"\"\"\n    num_batch, height, width, channels = im.shape.as_list()\n\n    x = tf.cast(x, 'float32') + 1  # Shift to [0, 2] range\n    y = tf.cast(y, 'float32') + 1\n\n    max_x = tf.cast(width - 1, 'int32')\n    max_y = tf.cast(height - 1, 'int32')\n\n    x0 = tf.cast(tf.floor(x), 'int32')\n    x1 = x0 + 1\n    y0 = tf.cast(tf.floor(y), 'int32')\n    y1 = y0 + 1\n\n    x0 = tf.clip_by_value(x0, 0, max_x)\n    x1 = tf.clip_by_value(x1, 0, max_x)\n    y0 = tf.clip_by_value(y0, 0, max_y)\n    y1 = tf.clip_by_value(y1, 0, max_y)\n\n    base = tf.repeat(\n        tf.range(num_batch) * height * width, \n        out_size[0] * out_size[1]\n    )\n    base_y0 = base + y0 * width\n    base_y1 = base + y1 * width\n    idx_a = base_y0 + x0\n    idx_b = base_y1 + x0\n    idx_c = base_y0 + x1\n    idx_d = base_y1 + x1\n\n    im_flat = tf.reshape(im, [-1, channels])\n    Ia = tf.gather(im_flat, idx_a)\n    Ib = tf.gather(im_flat, idx_b)\n    Ic = tf.gather(im_flat, idx_c)\n    Id = tf.gather(im_flat, idx_d)\n\n    wa = (x1 - x) * (y1 - y)\n    wb = (x1 - x) * (y - y0)\n    wc = (x - x0) * (y1 - y)\n    wd = (x - x0) * (y - y0)\n\n    output = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n    return tf.reshape(output, [num_batch, out_size[0], out_size[1], channels])\n\n\ndef _meshgrid(height, width):\n    x_linspace = tf.linspace(-1.0, 1.0, width)\n    y_linspace = tf.linspace(-1.0, 1.0, height)\n    x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n    return tf.stack([x_coordinates, y_coordinates, tf.ones_like(x_coordinates)], axis=-1)\n\n\ndef _transform(theta, input_dim, out_size):\n    num_batch = tf.shape(input_dim)[0]\n    out_height, out_width = out_size\n    grid = _meshgrid(out_height, out_width)\n    grid = tf.expand_dims(grid, axis=0)\n    grid = tf.tile(grid, [num_batch, 1, 1, 1])\n    grid = tf.reshape(grid, [num_batch, -1, 3])  # Flatten middle dimensions \n    transformed_grid = tf.matmul(theta, grid, transpose_b=True)\n    x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, -1, 1])\n    y_s = tf.slice(transformed_grid, [0, 0, 1], [-1, -1, 1])\n    return _interpolate(input_dim, tf.squeeze(x_s), tf.squeeze(y_s), out_size)\n\n\nclass SpatialTransformer(keras.layers.Layer):\n    def __init__(self, out_size, **kwargs):\n        self.out_size = out_size\n        super(SpatialTransformer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        inputs, theta = inputs\n        return _transform(theta, inputs, self.out_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.871150Z","iopub.status.idle":"2024-04-14T14:32:53.872024Z","shell.execute_reply.started":"2024-04-14T14:32:53.871798Z","shell.execute_reply":"2024-04-14T14:32:53.871820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/network/physics_models.py","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom pprint import pprint\nimport inspect\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nplt.switch_backend('agg')\n\nlogger = logging.getLogger(\"tf\")\n\nCELLS = {\n    \"bouncing_ode_cell\": BouncingODECell,\n    \"spring_ode_cell\": SpringODECell,\n    \"gravity_ode_cell\": GravityODECell,\n    \"lstm\": tf.keras.layers.LSTMCell\n}\n\n# total number of latent units for each datasets\n# coord_units = num_objects*num_dimensions*2\nCOORD_UNITS = {\n    \"bouncing_balls\": 8,\n    \"spring_color\": 8,\n    \"spring_color_half\": 8,\n    \"3bp_color\": 12,\n    \"mnist_spring_color\": 8\n}\n\nclass ConvEncoder(keras.layers.Layer):\n    def __init__(self, input_shape, n_objs, unet_type='unet', **kwargs):\n        super(ConvEncoder, self).__init__(**kwargs)\n        self.conv_input_shape = input_shape\n        self.n_objs = n_objs\n        self.unet_type = unet_type\n\n        if unet_type == 'unet':\n            self.downsampler = unet(tf.keras.layers.Input(input_shape), 16, self.n_objs + 1, upsamp=True)\n        elif unet_type == 'shallow_unet':\n            self.downsampler = shallow_unet(tf.keras.layers.Input(input_shape), 8, self.n_objs + 1, upsamp=True)\n        else:\n            raise ValueError(f\"Unsupported unet_type: {unet_type}\")\n\n        self.flatten = keras.layers.Flatten()\n        self.dense_layers = [\n            keras.layers.Dense(200, activation='relu'),\n            keras.layers.Dense(200, activation='relu'),\n            keras.layers.Dense(2, activation=None)\n        ]\n\n    def call(self, inputs):\n        h = self.downsampler(inputs)\n\n        # Object Mask Generation\n        h = keras.layers.Softmax(axis=-1)(h)\n        self.enc_masks = h\n        masked_objs = [h[..., i:i+1] * inputs for i in range(self.n_objs)]\n\n        # Encode Each Object\n        encoded_objs = []\n        for obj in masked_objs:\n            if self.conv_input_shape[0] >= 40:\n                obj = keras.layers.AveragePooling2D(2, 2)(obj)\n\n            obj = self.flatten(obj)\n            for layer in self.dense_layers:\n                obj = layer(obj)\n            encoded_objs.append(obj)\n\n        # Combine Encodings\n        h = keras.layers.concatenate(encoded_objs, axis=1)\n        h = tf.tanh(h) * (self.conv_input_shape[0] / 2) + (self.conv_input_shape[0] / 2)\n        return h  \n\nclass ConvSTDecoder(keras.layers.Layer):\n    def __init__(self, input_shape, n_objs, template_size, **kwargs):\n        super(ConvSTDecoder, self).__init__(**kwargs)\n        self.input_shape = input_shape\n        self.n_objs = n_objs\n        self.conv_input_shape = input_shape\n        self.template_size = template_size\n\n        # Network for creating templates, contents, and background\n        self.network = tf.keras.Sequential([\n            layers.Dense(200, activation='tanh'),\n            layers.Dense(np.prod([n_objs + 1] + input_shape), activation=None)  # +1 for background\n        ])\n\n        # Spatial Transformer Layer\n        self.stn_layer = SpacialTransformer(self.conv_input_shape[:2])  \n\n        # Parameter for spatial transformations\n        self.logsigma = self.add_weight(name='logsigma', shape=[], \n                                        initializer='zeros', trainable=True)\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n\n        # Generate Templates, Contents, and Background\n        templates_contents = self.network(tf.ones([1, 10]))  # Dummy input\n        templates_contents = tf.reshape(templates_contents, \n                                        [self.n_objs + 1, *self.input_shape])\n\n        template = templates_contents[0] + 5  # Use the first object as the template\n        template = tf.tile(template[None, ...], [batch_size, 1, 1, 1])\n        contents = tf.nn.sigmoid(templates_contents[1:-1])  # Object contents\n        background_content = tf.nn.sigmoid(templates_contents[-1:])  # Background content\n\n        # Spatial Transformation on Objects\n        transformed_objs = []\n        for obj_content, loc in zip(tf.split(contents, self.n_objs, axis=0), \n                                    tf.split(inputs, self.n_objs, axis=1)):\n            theta = self._get_theta(loc)\n            transformed_obj = self.stn_layer([obj_content, theta])\n            transformed_objs.append(transformed_obj)\n\n        # Combine with Background\n        contents = transformed_objs + [background_content]\n        masks = self._generate_masks(transformed_objs + [background_content]) \n        output = tf.add_n([m * c for m, c in zip(masks, contents)])\n\n        return output\n\n    def _get_theta(self, loc):\n        \"\"\"Calculates transformation parameters.\"\"\"\n        sigma = tf.exp(self.logsigma)\n        theta0 = tf.tile([[sigma]], [tf.shape(loc)[0]])\n        theta1 = tf.zeros_like(theta0)\n        theta2 = (self.conv_input_shape[0]/2 - loc[:,0]) / self.template_size * sigma\n        theta3 = tf.zeros_like(theta0)\n        theta4 = tf.tile([[sigma]], [tf.shape(loc)[0]])\n        theta5 = (self.conv_input_shape[0]/2 - loc[:,1]) / self.template_size * sigma\n        return tf.stack([theta0, theta1, theta2, theta3, theta4, theta5], axis=1)\n\n    def _generate_masks(self, contents):\n        \"\"\"Generates attention-like masks for blending contents and background.\"\"\"\n        all_contents = tf.concat(contents, axis=-1)\n        masks = tf.nn.softmax(all_contents - 5, axis=-1)\n        return tf.split(masks, self.n_objs + 1, axis=-1) \n\nclass PhysicsNet(BaseNet):\n    def __init__(self,\n                 task=\"spring_color\",\n                 recurrent_units=128,\n                 lstm_layers=1,\n                 cell_type=\"\",\n                 seq_len=20,\n                 input_steps=3,\n                 pred_steps=5,\n                 autoencoder_loss=3.0,\n                 alt_vel=False,\n                 color=False,\n                 input_size=36*36,\n                 encoder_type=\"conv_encoder\",\n                 decoder_type=\"conv_st_decoder\"):\n        \n        super(PhysicsNet, self).__init__(**kwargs)\n        \n        assert task in COORD_UNITS\n        self.task = task\n        \n        # Only used when using black-box dynamics (baselines)\n        self.recurrent_units = recurrent_units\n        self.lstm_layers = lstm_layers\n        self.lstm = tf.keras.layers.LSTMCell(self.recurrent_units)\n        \n        self.cell_type = cell_type\n        self.cell = CELLS[self.cell_type]\n        self.coord_units = coord_units \n        self.input_shape = input_shape \n        self.encoder = ConvEncoder()\n        self.decoder = ConvSTDecoder()\n        self.ode_cell = self.cell(self.coord_units // 2)\n        \n        assert seq_len > input_steps + pred_steps\n        assert input_steps >= 1\n        assert pred_steps >= 1\n        \n        self.seq_len = seq_len\n        self.input_steps = input_steps\n        self.pred_steps = pred_steps\n        self.extrap_steps = self.seq_len-self.input_steps-self.pred_steps\n\n        self.alt_vel = alt_vel\n        self.autoencoder_loss = autoencoder_loss\n\n        self.coord_units = COORD_UNITS[self.task]\n        self.n_objs = self.coord_units//4\n\n        self.extra_valid_fns.append((self.visualize_sequence,[],{}))\n        self.extra_test_fns.append((self.visualize_sequence,[],{}))\n        \n    def build_optimizer(self, base_lr, optimizer=\"rmsprop\", anneal_lr=True):\n        # Uncomment lines below to have different learning rates for physics and vision components\n        self.base_lr = base_lr\n        self.anneal_lr = anneal_lr\n        if optimizer == 'adam':\n            self.optimizer = tf.keras.optimizers.Adam(learning_rate=base_lr)\n        elif optimizer == 'rmsprop':\n            self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_lr)\n        elif optimizer == 'momentum':\n            self.optimizer = lambda lr: tf.keras.optimizers.SGD(lr, momentum=0.9),\n        elif optimizer == 'sgd':\n            self.optimizer = tf.keras.optimizers.SGD\n            \n        self.lr = tf.Variable(base_lr, trainable=False, name=\"base_lr\")\n        self.optimizer = OPTIMIZERS[optimizer](self.lr)\n        #self.dyn_optimizer = OPTIMIZERS[optimizer](1e-3)\n\n        update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            gvs = self.optimizer.compute_gradients(self.loss, var_list=tf.compat.v1.trainable_variables())\n            gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs if grad is not None]\n            self.train_op = self.optimizer.apply_gradients(gvs)\n        \n    \n    def call(self, inputs):\n        enc_pos = self.encoder(inputs)\n        # ... (Rollout ODE and decode)\n#         lstms = [tf.keras.layers.LSTMCell(self.recurrent_units) for i in range(self.lstm_layers)]\n#         states = [lstm.zero_state(tf.shape(self.input)[0], dtype=tf.float32) for lstm in lstms]\n#         rollout_cell = self.cell(self.coord_units//2)\n        \n        \n        x = self.lstm(enc_pos)\n        \n        \n        # decode the input and pred frames\n        recons_out = self.decoder(enc_pos)\n\n        self.recons_out = tf.reshape(recons_out, \n                                     [tf.shape(self.input)[0], self.input_steps+self.pred_steps]+self.input_shape)\n        self.enc_pos = tf.reshape(enc_pos, \n                                  [tf.shape(self.input)[0], self.input_steps+self.pred_steps, self.coord_units//2])\n\n        if self.input_steps > 1:\n            vel = self.vel_encoder(self.enc_pos[:,:self.input_steps], scope=tvs)\n        else:\n            vel = tf.zeros([tf.shape(self.input)[0], self.coord_units//2])\n\n        pos = self.enc_pos[:,self.input_steps-1]\n        output_seq = []\n        pos_vel_seq = []\n        pos_vel_seq.append(tf.concat([pos, vel], axis=1))\n\n        # rollout ODE and decoder\n        for t in range(self.pred_steps+self.extrap_steps):\n            # rollout\n            pos, vel = rollout_cell(pos, vel)\n\n            # decode\n            out = self.decoder(pos, scope=tvs)\n\n            pos_vel_seq.append(tf.concat([pos, vel], axis=1))\n            output_seq.append(out)\n\n        current_scope = tf.compat.v1.get_default_graph().get_name_scope()\n        self.network_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \n                                              scope=current_scope)\n        logger.info(self.network_vars)\n        \n        output_seq = tf.stack(output_seq)\n        pos_vel_seq = tf.stack(pos_vel_seq)\n        output_seq = tf.transpose(output_seq, (1,0,2,3,4))\n        self.pos_vel_seq = tf.transpose(pos_vel_seq, (1,0,2))\n        return output_seq\n    \n    def get_batch(self, batch_size, iterator):\n        batch_x, _ = iterator.next_batch(batch_size)\n        batch_len = batch_x.shape[1]\n        feed_dict = {self.input: batch_x}\n        return feed_dict, (batch_x, None)\n\n    def compute_loss(self):\n\n        # Compute reconstruction loss\n        recons_target = self.input[:,:self.input_steps+self.pred_steps]\n        recons_loss = tf.square(recons_target-self.recons_out)\n        #recons_ce_loss = -(recons_target*tf.log(self.recons_out+1e-7) + (1.0-recons_target)*tf.log(1.0-self.recons_out+1e-7))\n        recons_loss = tf.reduce_sum(recons_loss, axis=[2,3,4])\n\n        self.recons_loss = tf.reduce_mean(recons_loss)\n\n        target = self.input[:,self.input_steps:]\n        #ce_loss = -(target*tf.log(self.output+1e-7) + (1.0-target)*tf.log(1.0-self.output+1e-7))\n        loss = tf.square(target-self.output)\n        loss = tf.reduce_sum(loss, axis=[2,3,4])\n\n        # Compute prediction losses. pred_loss is used for training, extrap_loss is used for evaluation\n        self.pred_loss = tf.reduce_mean(loss[:,:self.pred_steps])\n        self.extrap_loss = tf.reduce_mean(loss[:,self.pred_steps:])\n\n        train_loss = self.pred_loss\n        if self.autoencoder_loss > 0.0:\n            train_loss += self.autoencoder_loss*self.recons_loss\n\n        eval_losses = [self.pred_loss, self.extrap_loss, self.recons_loss]\n        return train_loss, eval_losses\n    \n    def visualize_sequence(self):\n        batch_size = 5\n\n        feed_dict, (batch_x, _) = self.get_batch(batch_size, self.test_iterator)\n        fetches = [self.output, self.recons_out]\n        if hasattr(self, 'pos_vel_seq'):\n            fetches.append(self.pos_vel_seq)\n\n        res = self.sess.run(fetches, feed_dict=feed_dict)\n        output_seq = res[0]\n        recons_seq = res[1]\n        if hasattr(self, 'pos_vel_seq'):\n            pos_vel_seq = res[2]\n        output_seq = np.concatenate([batch_x[:,:self.input_steps], output_seq], axis=1)\n        recons_seq = np.concatenate([recons_seq, np.zeros((batch_size, self.extrap_steps)+recons_seq.shape[2:])], axis=1)\n\n        # Plot a grid with prediction sequences\n        for i in range(batch_x.shape[0]):\n            #if hasattr(self, 'pos_vel_seq'):\n            #    if i == 0 or i == 1:\n            #        logger.info(pos_vel_seq[i])\n\n            to_concat = [output_seq[i],batch_x[i],recons_seq[i]]\n            total_seq = np.concatenate(to_concat, axis=0) \n\n            total_seq = total_seq.reshape([total_seq.shape[0], \n                                           self.input_shape[0], \n                                           self.input_shape[1], self.conv_ch])\n\n            result = gallery(total_seq, ncols=batch_x.shape[1])\n\n            norm = plt.Normalize(0.0, 1.0)\n\n            figsize = (result.shape[1]//self.input_shape[1], result.shape[0]//self.input_shape[0])\n            fig, ax = plt.subplots(figsize=figsize)\n            ax.imshow(np.squeeze(result), interpolation='nearest', cmap=cm.Greys_r, norm=norm)\n            ax.get_xaxis().set_visible(False)\n            ax.get_yaxis().set_visible(False)\n            fig.tight_layout()\n            fig.savefig(os.path.join(self.save_dir, \"example%d.png\"%i))\n\n        # Make a gif from the sequences\n        bordered_output_seq = 0.5*np.ones([batch_size, self.seq_len, \n                                          self.conv_input_shape[0]+2, self.conv_input_shape[1]+2, 3])\n        bordered_batch_x = 0.5*np.ones([batch_size, self.seq_len, \n                                          self.conv_input_shape[0]+2, self.conv_input_shape[1]+2, 3])\n        output_seq = output_seq.reshape([batch_size, self.seq_len]+self.input_shape)\n        batch_x = batch_x.reshape([batch_size, self.seq_len]+self.input_shape)\n        bordered_output_seq[:,:,1:-1,1:-1] = output_seq\n        bordered_batch_x[:,:,1:-1,1:-1] = batch_x\n        output_seq = bordered_output_seq\n        batch_x = bordered_batch_x\n        output_seq = np.concatenate(np.split(output_seq, batch_size, 0), axis=-2).squeeze()\n        batch_x = np.concatenate(np.split(batch_x, batch_size, 0), axis=-2).squeeze()\n        frames = np.concatenate([output_seq, batch_x], axis=1)\n\n        gif(os.path.join(self.save_dir, \"animation%d.gif\"%i), \n            frames*255, fps=7, scale=3)\n\n        # Save extra tensors for visualization\n        fetches = {\"contents\": self.contents,\n                   \"templates\": self.template,\n                   \"background_content\": self.background_content,\n                   \"transf_contents\": self.transf_contents,\n                   \"transf_masks\": self.transf_masks,\n                   \"enc_masks\": self.enc_masks,\n                   \"masked_objs\": self.masked_objs}\n        results = self.sess.run(fetches, feed_dict=feed_dict)\n        np.savez_compressed(os.path.join(self.save_dir, \"extra_outputs.npz\"), **results)\n        contents = results[\"contents\"]\n        templates = results[\"templates\"]\n        contents = 1/(1+np.exp(-contents))\n        templates = 1/(1+np.exp(-(templates-5)))\n        if self.conv_ch == 1:\n            contents = np.tile(contents, [1,1,1,3])\n        templates = np.tile(templates, [1,1,1,3])\n        total_seq = np.concatenate([contents, templates], axis=0)\n        result = gallery(total_seq, ncols=self.n_objs)\n        fig, ax = plt.subplots(figsize=figsize)\n        ax.imshow(np.squeeze(result), interpolation='nearest', cmap=cm.Greys_r, norm=norm)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        fig.tight_layout()\n        fig.savefig(os.path.join(self.save_dir, \"templates.png\"))\n\n        logger.info([(v.name, self.sess.run(v)) for v in tf.compat.v1.trainable_variables() if \"ode_cell\" in v.name or \"sigma\" in v.name])\n        return\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.873607Z","iopub.status.idle":"2024-04-14T14:32:53.874295Z","shell.execute_reply.started":"2024-04-14T14:32:53.874064Z","shell.execute_reply":"2024-04-14T14:32:53.874085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/datasets/generators.py","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom itertools import combinations\n\n# from nn.utils.viz import gallery\n# from nn.utils.misc import rgb2gray\n\ndef generate_bouncing_ball_dataset(dest,\n                                   train_set_size,\n                                   valid_set_size,\n                                   test_set_size,\n                                   seq_len,\n                                   box_size):\n    np.random.seed(0)\n\n    def verify_collision(x, v):\n        if x[0] + v[0] > box_size or x[0] + v[0] < 0.0:\n            v[0] = -v[0]\n        if x[1] + v[1] > box_size or x[1] + v[1] < 0.0:\n            v[1] = -v[1]\n        return v\n\n    def generate_trajectory(steps):\n        traj = []\n        x = np.random.rand(2)*box_size\n        speed = np.random.rand()+1\n        angle = np.random.rand()*2*np.pi\n        v = np.array([speed*np.cos(angle), speed*np.sin(angle)])\n        for _ in range(steps):\n            traj.append(x)\n            v = verify_collision(x, v)\n            x = x + v\n        return traj\n\n    trajectories = []\n    for i in range(train_set_size+valid_set_size+test_set_size):\n        trajectories.append(generate_trajectory(seq_len))\n    trajectories = np.array(trajectories)\n\n    np.savez_compressed(dest, \n                        train_x=trajectories[:train_set_size],\n                        valid_x=trajectories[train_set_size:train_set_size+valid_set_size],\n                        test_x=trajectories[train_set_size+valid_set_size:])\n    print(\"Saved to file %s\" % dest)\n\n\ndef compute_wall_collision(pos, vel, radius, img_size):\n    if pos[1]-radius <= 0:\n        vel[1] = -vel[1]\n        pos[1] = -(pos[1]-radius)+radius\n    if pos[1]+radius >= img_size[1]:\n        vel[1] = -vel[1]\n        pos[1] = img_size[1]-(pos[1]+radius-img_size[1])-radius  \n    if pos[0]-radius <= 0:\n        vel[0] = -vel[0]\n        pos[0] = -(pos[0]-radius)+radius\n    if pos[0]+radius >= img_size[0]:\n        vel[0] = -vel[0]\n        pos[0] = img_size[0]-(pos[0]+radius-img_size[0])-radius \n    return pos, vel\n\n\ndef verify_wall_collision(pos, vel, radius, img_size):\n    if pos[1]-radius <= 0:\n        return True\n    if pos[1]+radius >= img_size[1]:\n        return True \n    if pos[0]-radius <= 0:\n        return True\n    if pos[0]+radius >= img_size[0]:\n        return True\n    return False\n\n\ndef verify_object_collision(poss, radius):\n    for pos1, pos2 in combinations(poss, 2):\n        if np.linalg.norm(pos1-pos2) <= radius:\n            return True\n    return False\n\n\ndef generate_falling_ball_dataset(dest,\n                                  train_set_size,\n                                  valid_set_size,\n                                  test_set_size,\n                                  seq_len,\n                                  img_size=None,\n                                  radius=3,\n                                  dt=0.15,\n                                  g=9.8,\n                                  ode_steps=10):\n\n    from skimage.draw import circle\n    from nn.utils.viz import gallery\n    import matplotlib.cm as cm\n    if img_size is None:\n        img_size = [32,32]\n\n    def generate_sequence():\n        seq = []\n        # sample initial position, with v=0\n        pos = np.random.rand(2)\n        pos[0] = radius+(img_size[0]-2*radius)*pos[0]\n        pos[1] = radius + (img_size[1]-2*radius)/2*pos[1]\n        vel = np.array([0.0,0.0])\n\n        for i in range(seq_len):\n            assert pos[1]+radius < img_size[1]\n\n            frame = np.zeros(img_size+[1], dtype=np.int8)\n            rr, cc = circle(int(pos[1]), int(pos[0]), radius)\n            frame[rr, cc, 0] = 255\n\n            seq.append(frame)\n\n            # rollout physics\n            for _ in range(ode_steps):\n                vel[1] = vel[1] + dt/ode_steps*g\n                pos[1] = pos[1] + dt/ode_steps*vel[1]    \n\n        return seq\n    \n    sequences = []\n    for i in range(train_set_size+valid_set_size+test_set_size):\n        if i % 100 == 0:\n            print(i)\n        sequences.append(generate_sequence())\n    sequences = np.array(sequences, dtype=np.uint8)\n\n    np.savez_compressed(dest, \n                        train_x=sequences[:train_set_size],\n                        valid_x=sequences[train_set_size:train_set_size+valid_set_size],\n                        test_x=sequences[train_set_size+valid_set_size:])\n    print(\"Saved to file %s\" % dest)\n\n    # Save 10 samples\n    result = gallery(np.concatenate(sequences[:10]/255), ncols=sequences.shape[1])\n\n    norm = plt.Normalize(0.0, 1.0)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.imshow(np.squeeze(result), interpolation='nearest', cmap=cm.Greys_r, norm=norm)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    fig.tight_layout()\n    fig.savefig(dest.split(\".\")[0]+\"_samples.jpg\")\n\n\ndef generate_falling_bouncing_ball_dataset(dest,\n                                  train_set_size,\n                                  valid_set_size,\n                                  test_set_size,\n                                  seq_len,\n                                  img_size=None,\n                                  radius=3,\n                                  dt=0.30,\n                                  g=9.8,\n                                  vx0_max=0.0,\n                                  vy0_max=0.0,\n                                  cifar_background=False,\n                                  ode_steps=10):\n\n    if cifar_background:\n        import tensorflow as tf\n        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n    from skimage.draw import circle\n    from skimage.transform import resize\n\n    if img_size is None:\n        img_size = [32,32]\n    scale = 10\n    scaled_img_size = [img_size[0]*scale, img_size[1]*scale]\n\n    def generate_sequence():\n        seq = []\n        # sample initial position, with v=0\n        pos = np.random.rand(2)\n        pos[0] = radius + (img_size[0]-2*radius)*pos[0]\n        if g == 0.0:\n            pos[1] = radius + (img_size[1]-2*radius)*pos[1]\n        else:\n            pos[1] = radius + (img_size[1]-2*radius)/2*pos[1]\n        angle = np.random.rand()*2*np.pi\n        vel = np.array([np.cos(angle)*vx0_max, \n                        np.sin(angle)*vy0_max])\n\n        if cifar_background:\n            cifar_img = x_train[np.random.randint(50000)]\n\n        for i in range(seq_len):\n            if cifar_background:\n                frame = cifar_img\n                frame = rgb2gray(frame)/255\n                frame = resize(frame, scaled_img_size)\n                frame = np.clip(frame-0.2, 0.0, 1.0) # darken image a bit\n            else:\n                frame = np.zeros(scaled_img_size, dtype=np.float32)\n\n            rr, cc = circle(int(pos[1]*scale), int(pos[0]*scale), radius*scale, scaled_img_size)\n            frame[rr, cc] = 1.0\n            frame = resize(frame, img_size, anti_aliasing=True)\n            frame = (frame[:,:,None]*255).astype(np.uint8)\n\n            seq.append(frame)\n\n            # rollout physics\n            for _ in range(ode_steps):\n                vel[1] = vel[1] + dt/ode_steps*g\n                pos[1] = pos[1] + dt/ode_steps*vel[1]\n\n                pos[0] = pos[0] + dt/ode_steps*vel[0]\n\n                # verify wall collisions\n                pos, vel = compute_wall_collision(pos, vel, radius, img_size)\n        return seq\n    \n    sequences = []\n    for i in range(train_set_size+valid_set_size+test_set_size):\n        if i % 100 == 0:\n            print(i)\n        sequences.append(generate_sequence())\n    sequences = np.array(sequences, dtype=np.uint8)\n\n    np.savez_compressed(dest, \n                        train_x=sequences[:train_set_size],\n                        valid_x=sequences[train_set_size:train_set_size+valid_set_size],\n                        test_x=sequences[train_set_size+valid_set_size:])\n    print(\"Saved to file %s\" % dest)\n\n    # Save 10 samples\n    result = gallery(np.concatenate(sequences[:10]/255), ncols=sequences.shape[1])\n\n    norm = plt.Normalize(0.0, 1.0)\n    fig, ax = plt.subplots(figsize=(sequences.shape[1], 10))\n    ax.imshow(np.squeeze(result), interpolation='nearest', cmap=cm.Greys_r, norm=norm)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    fig.tight_layout()\n    fig.savefig(dest.split(\".\")[0]+\"_samples.jpg\")\n\n\ndef generate_spring_balls_dataset(dest,\n                                  train_set_size,\n                                  valid_set_size,\n                                  test_set_size,\n                                  seq_len,\n                                  img_size=None,\n                                  radius=3,\n                                  dt=0.3,\n                                  k=3,\n                                  equil=5,\n                                  vx0_max=0.0,\n                                  vy0_max=0.0,\n                                  color=False,\n                                  cifar_background=False,\n                                  ode_steps=10):\n\n    if cifar_background:\n        import tensorflow as tf\n        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n    from skimage.draw import circle\n    from skimage.transform import resize\n\n    if img_size is None:\n        img_size = [32,32]\n    scale = 10\n    scaled_img_size = [img_size[0]*scale, img_size[1]*scale]\n\n    def generate_sequence():\n        # sample initial position of the center of mass, then sample\n        # position of each object relative to that.\n\n        collision = True\n        while collision == True:\n            seq = []\n\n            cm_pos = np.random.rand(2)\n            cm_pos[0] = radius+equil + (img_size[0]-2*(radius+equil))*cm_pos[0]\n            cm_pos[1] = radius+equil + (img_size[1]-2*(radius+equil))*cm_pos[1]\n\n            angle = np.random.rand()*2*np.pi\n            # calculate position of both objects\n            r = np.random.rand()+0.5\n            poss = [[np.cos(angle)*equil*r+cm_pos[0], np.sin(angle)*equil*r+cm_pos[1]],\n                   [np.cos(angle+np.pi)*equil*r+cm_pos[0], np.sin(angle+np.pi)*equil*r+cm_pos[1]]]\n            poss = np.array(poss)\n            angles = np.random.rand(2)*2*np.pi\n            vels = [[np.cos(angles[0])*vx0_max, np.sin(angles[0])*vy0_max],\n                   [np.cos(angles[1])*vx0_max, np.sin(angles[1])*vy0_max]]\n            vels = np.array(vels)\n\n            if cifar_background:\n                cifar_img = x_train[np.random.randint(50000)]\n\n            for i in range(seq_len):\n                if cifar_background:\n                    frame = cifar_img\n                    frame = rgb2gray(frame)/255\n                    frame = resize(frame, scaled_img_size)\n                    frame = np.clip(frame-0.2, 0.0, 1.0) # darken image a bit\n                else:\n                    if color:\n                        frame = np.zeros(scaled_img_size+[3], dtype=np.float32)\n                    else:\n                        frame = np.zeros(scaled_img_size+[1], dtype=np.float32)\n\n\n                for j, pos in enumerate(poss):\n                    rr, cc = circle(int(pos[1]*scale), int(pos[0]*scale), radius*scale, scaled_img_size)\n                    if color:\n                        frame[rr, cc, 2-j] = 1.0 \n                    else:\n                        frame[rr, cc, 0] = 1.0 \n\n                frame = resize(frame, img_size, anti_aliasing=True)\n                frame = (frame*255).astype(np.uint8)\n\n                seq.append(frame)\n\n                # rollout physics\n                for _ in range(ode_steps):\n                    norm = np.linalg.norm(poss[0]-poss[1])\n                    direction = (poss[0]-poss[1])/norm\n                    F = k*(norm-2*equil)*direction\n                    vels[0] = vels[0] - dt/ode_steps*F\n                    vels[1] = vels[1] + dt/ode_steps*F\n                    poss = poss + dt/ode_steps*vels\n\n                    collision = verify_wall_collision(poss[0], vels[0], radius, img_size) or \\\n                                verify_wall_collision(poss[1], vels[1], radius, img_size)\n                    if collision:\n                        break\n                    #poss[0], vels[0] = compute_wall_collision(poss[0], vels[0], radius, img_size)\n                    #poss[1], vels[1] = compute_wall_collision(poss[1], vels[1], radius, img_size)\n                if collision:\n                    break\n\n        return seq\n    \n    sequences = []\n    for i in range(train_set_size+valid_set_size+test_set_size):\n        if i % 100 == 0:\n            print(i)\n        sequences.append(generate_sequence())\n    sequences = np.array(sequences, dtype=np.uint8)\n\n    np.savez_compressed(dest, \n                        train_x=sequences[:train_set_size],\n                        valid_x=sequences[train_set_size:train_set_size+valid_set_size],\n                        test_x=sequences[train_set_size+valid_set_size:])\n    print(\"Saved to file %s\" % dest)\n\n    # Save 10 samples\n    result = gallery(np.concatenate(sequences[:10]/255), ncols=sequences.shape[1])\n\n    norm = plt.Normalize(0.0, 1.0)\n    fig, ax = plt.subplots(figsize=(sequences.shape[1], 10))\n    ax.imshow(np.squeeze(result), interpolation='nearest', cmap=cm.Greys_r, norm=norm)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    fig.tight_layout()\n    fig.savefig(dest.split(\".\")[0]+\"_samples.jpg\")\n\n\ndef generate_spring_mnist_dataset(dest,\n                                  train_set_size,\n                                  valid_set_size,\n                                  test_set_size,\n                                  seq_len,\n                                  img_size=None,\n                                  radius=3,\n                                  dt=0.3,\n                                  k=3,\n                                  equil=5,\n                                  vx0_max=0.0,\n                                  vy0_max=0.0,\n                                  color=False,\n                                  cifar_background=False,\n                                  ode_steps=10):\n\n    # A single CIFAR image is used for background\n    # Only 2 mnist digits are used\n    import tensorflow as tf\n    from skimage.draw import circle\n    from skimage.transform import resize\n\n    scale = 5\n    if img_size is None:\n        img_size = [32,32]    \n    scaled_img_size = [img_size[0]*scale, img_size[1]*scale]\n\n    if cifar_background:\n        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n        cifar_img = x_train[1]\n        \n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n    digits = x_train[0:2, 3:-3, 3:-3]/255\n    digits = [resize(d, [22*scale, 22*scale]) for d in digits]\n    radius = 11\n\n    def generate_sequence():\n        # sample initial position of the center of mass, then sample\n        # position of each object relative to that.\n\n        collision = True\n        while collision == True:\n            seq = []\n\n            cm_pos = np.random.rand(2)\n            cm_pos[0] = radius+equil + (img_size[0]-2*(radius+equil))*cm_pos[0]\n            cm_pos[1] = radius+equil + (img_size[1]-2*(radius+equil))*cm_pos[1]\n\n            angle = np.random.rand()*2*np.pi\n            # calculate position of both objects\n            r = np.random.rand()+0.5\n            poss = [[np.cos(angle)*equil*r+cm_pos[0], np.sin(angle)*equil*r+cm_pos[1]],\n                   [np.cos(angle+np.pi)*equil*r+cm_pos[0], np.sin(angle+np.pi)*equil*r+cm_pos[1]]]\n            poss = np.array(poss)\n            angles = np.random.rand(2)*2*np.pi\n            vels = [[np.cos(angles[0])*vx0_max, np.sin(angles[0])*vy0_max],\n                   [np.cos(angles[1])*vx0_max, np.sin(angles[1])*vy0_max]]\n            vels = np.array(vels)\n\n            for i in range(seq_len):\n                if cifar_background:\n                    frame = cifar_img\n                    if not color:\n                        frame = rgb2gray(frame)\n                        frame = frame[:,:,None]\n                    frame = frame/255\n                    frame = resize(frame, scaled_img_size)\n                    frame = np.clip(frame-0.2, 0.0, 1.0) # darken image a bit\n                else:\n                    if color:\n                        frame = np.zeros(scaled_img_size+[3], dtype=np.float32)\n                    else:\n                        frame = np.zeros(scaled_img_size+[1], dtype=np.float32)\n\n\n                for j, pos in enumerate(poss):\n                    rr, cc = circle(int(pos[1]*scale), int(pos[0]*scale), radius*scale, scaled_img_size)\n                    frame_coords = np.array([[max(0, (pos[1]-radius)*scale), min(scaled_img_size[1], (pos[1]+radius)*scale)],\n                                             [max(0, (pos[0]-radius)*scale), min(scaled_img_size[0], (pos[0]+radius)*scale)]])\n                    digit_coords = np.array([[max(0, (radius-pos[1])*scale), min(2*radius*scale, scaled_img_size[1]-(pos[1]-radius)*scale)],\n                                             [max(0, (radius-pos[0])*scale), min(2*radius*scale, scaled_img_size[0]-(pos[0]-radius)*scale)]])\n                    frame_coords = np.round(frame_coords).astype(np.int32)\n                    digit_coords = np.round(digit_coords).astype(np.int32)\n                    \n                    digit_slice = digits[j][digit_coords[0,0]:digit_coords[0,1], \n                                            digit_coords[1,0]:digit_coords[1,1]]\n                    if color:\n                        for l in range(3):\n                            frame_slice = frame[frame_coords[0,0]:frame_coords[0,1], \n                                                frame_coords[1,0]:frame_coords[1,1], l]\n                            c = 1.0 if l == j else 0.0\n                            frame[frame_coords[0,0]:frame_coords[0,1], \n                                  frame_coords[1,0]:frame_coords[1,1], l] = digit_slice*c + (1-digit_slice)*frame_slice\n\n                    else:\n                        frame_slice = frame[frame_coords[0,0]:frame_coords[0,1], \n                                            frame_coords[1,0]:frame_coords[1,1], 0]\n                        frame[frame_coords[0,0]:frame_coords[0,1], \n                              frame_coords[1,0]:frame_coords[1,1], 0] = digit_slice + (1-digit_slice)*frame_slice\n\n                frame = resize(frame, img_size, anti_aliasing=True)\n                frame = (frame*255).astype(np.uint8)\n\n                seq.append(frame)\n\n                # rollout physics\n                for _ in range(ode_steps):\n                    norm = np.linalg.norm(poss[0]-poss[1])\n                    direction = (poss[0]-poss[1])/norm\n                    F = k*(norm-2*equil)*direction\n                    vels[0] = vels[0] - dt/ode_steps*F\n                    vels[1] = vels[1] + dt/ode_steps*F\n                    poss = poss + dt/ode_steps*vels\n\n                    collision = verify_wall_collision(poss[0], vels[0], 2, img_size) or \\\n                                verify_wall_collision(poss[1], vels[1], 2, img_size)\n                    if collision:\n                        break\n                    #poss[0], vels[0] = compute_wall_collision(poss[0], vels[0], radius, img_size)\n                    #poss[1], vels[1] = compute_wall_collision(poss[1], vels[1], radius, img_size)\n                if collision:\n                    break\n\n        return seq\n    \n    sequences = []\n    for i in range(train_set_size+valid_set_size+test_set_size):\n        if i % 100 == 0:\n            print(i)\n        sequences.append(generate_sequence())\n    sequences = np.array(sequences, dtype=np.uint8)\n\n    np.savez_compressed(dest, \n                        train_x=sequences[:train_set_size],\n                        valid_x=sequences[train_set_size:train_set_size+valid_set_size],\n                        test_x=sequences[train_set_size+valid_set_size:])\n    print(\"Saved to file %s\" % dest)\n\n    # Save 10 samples\n    result = gallery(np.concatenate(sequences[:10]/255), ncols=sequences.shape[1])\n\n    norm = plt.Normalize(0.0, 1.0)\n    fig, ax = plt.subplots(figsize=(sequences.shape[1], 10))\n    ax.imshow(np.squeeze(result), interpolation='nearest', cmap=cm.Greys_r, norm=norm)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    fig.tight_layout()\n    fig.savefig(dest.split(\".\")[0]+\"_samples.jpg\")\n\n\ndef generate_3_body_problem_dataset(dest,\n                                  train_set_size,\n                                  valid_set_size,\n                                  test_set_size,\n                                  seq_len,\n                                  img_size=None,\n                                  radius=3,\n                                  dt=0.3,\n                                  g=9.8,\n                                  m=1.0,\n                                  vx0_max=0.0,\n                                  vy0_max=0.0,\n                                  color=False,\n                                  cifar_background=False,\n                                  ode_steps=10):\n\n    if cifar_background:\n        import tensorflow as tf\n        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n    from skimage.draw import circle\n    from skimage.transform import resize\n\n    if img_size is None:\n        img_size = [32,32]\n    scale = 10\n    scaled_img_size = [img_size[0]*scale, img_size[1]*scale]\n\n    def generate_sequence():\n        # sample initial position of the center of mass, then sample\n        # position of each object relative to that.\n\n        collision = True\n        while collision == True:\n            seq = []\n\n            cm_pos = np.random.rand(2)\n            cm_pos = np.array(img_size)/2\n            angle1 = np.random.rand()*2*np.pi\n            angle2 = angle1 + 2*np.pi/3+(np.random.rand()-0.5)/2\n            angle3 = angle1 + 4*np.pi/3+(np.random.rand()-0.5)/2\n\n            angles = [angle1, angle2, angle3]\n            # calculate position of both objects\n            r = (np.random.rand()/2+0.75)*img_size[0]/4\n            poss = [[np.cos(angle)*r+cm_pos[0], np.sin(angle)*r+cm_pos[1]] for angle in angles]\n            poss = np.array(poss)\n            \n            #angles = np.random.rand(3)*2*np.pi\n            #vels = [[np.cos(angle)*vx0_max, np.sin(angle)*vy0_max] for angle in angles]\n            #vels = np.array(vels)\n            r = np.random.randint(0,2)*2-1\n            angles = [angle+r*np.pi/2 for angle in angles]\n            noise = np.random.rand(2)-0.5\n            vels = [[np.cos(angle)*vx0_max+noise[0], np.sin(angle)*vy0_max+noise[1]] for angle in angles]\n            vels = np.array(vels)\n\n            if cifar_background:\n                cifar_img = x_train[np.random.randint(50000)]\n\n            for i in range(seq_len):\n                if cifar_background:\n                    frame = cifar_img\n                    frame = rgb2gray(frame)/255\n                    frame = resize(frame, scaled_img_size)\n                    frame = np.clip(frame-0.2, 0.0, 1.0) # darken image a bit\n                else:\n                    if color:\n                        frame = np.zeros(scaled_img_size+[3], dtype=np.float32)\n                    else:\n                        frame = np.zeros(scaled_img_size+[1], dtype=np.float32)\n\n                for j, pos in enumerate(poss):\n                    rr, cc = circle(int(pos[1]*scale), int(pos[0]*scale), radius*scale, scaled_img_size)\n                    if color:\n                        frame[rr, cc, 2-j] = 1.0 \n                    else:\n                        frame[rr, cc, 0] = 1.0 \n\n                frame = resize(frame, img_size, anti_aliasing=True)\n                frame = (frame*255).astype(np.uint8)\n\n                seq.append(frame)\n\n                # rollout physics\n                for _ in range(ode_steps):\n                    norm01 = np.linalg.norm(poss[0]-poss[1])\n                    norm12 = np.linalg.norm(poss[1]-poss[2])\n                    norm20 = np.linalg.norm(poss[2]-poss[0])\n                    vec01 = (poss[0]-poss[1])\n                    vec12 = (poss[1]-poss[2])\n                    vec20 = (poss[2]-poss[0])\n\n                    # Compute force vectors\n                    F = [vec01/norm01**3-vec20/norm20**3,\n                         vec12/norm12**3-vec01/norm01**3,\n                         vec20/norm20**3-vec12/norm12**3]\n                    F = np.array(F)\n                    F = -g*m*m*F\n\n                    vels = vels + dt/ode_steps*F\n                    poss = poss + dt/ode_steps*vels\n\n                    collision = any([verify_wall_collision(pos, vel, radius, img_size) for pos, vel in zip(poss, vels)]) or \\\n                                verify_object_collision(poss, radius+1)\n                    if collision:\n                        break\n\n                if collision:\n                    break\n\n        return seq\n    \n    sequences = []\n    for i in range(train_set_size+valid_set_size+test_set_size):\n        if i % 100 == 0:\n            print(i)\n        sequences.append(generate_sequence())\n    sequences = np.array(sequences, dtype=np.uint8)\n\n    np.savez_compressed(dest, \n                        train_x=sequences[:train_set_size],\n                        valid_x=sequences[train_set_size:train_set_size+valid_set_size],\n                        test_x=sequences[train_set_size+valid_set_size:])\n    print(\"Saved to file %s\" % dest)\n\n    # Save 10 samples\n    result = gallery(np.concatenate(sequences[:10]/255), ncols=sequences.shape[1])\n\n    norm = plt.Normalize(0.0, 1.0)\n    fig, ax = plt.subplots(figsize=(sequences.shape[1], 10))\n    ax.imshow(np.squeeze(result), interpolation='nearest', cmap=cm.Greys_r, norm=norm)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    fig.tight_layout()\n    fig.savefig(dest.split(\".\")[0]+\"_samples.jpg\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.875882Z","iopub.status.idle":"2024-04-14T14:32:53.876545Z","shell.execute_reply.started":"2024-04-14T14:32:53.876329Z","shell.execute_reply":"2024-04-14T14:32:53.876347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nn/datasets/iterators.py","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport tensorflow as tf\n\nclass DataIterator:\n\n    def __init__(self, X, Y=None):\n        self.X = X\n        self.Y = Y\n\n        self.num_examples = self.X.shape[0]\n        self.epochs_completed = 0\n        self.indices = np.arange(self.num_examples)\n        self.reset_iteration()\n\n    def reset_iteration(self):\n        np.random.shuffle(self.indices)\n        self.start_idx = 0\n\n    def get_epoch(self):\n        return self.epochs_completed\n\n    def reset_epoch(self):\n        self.reset_iteration()\n        self.epochs_completed = 0\n\n    def next_batch(self, batch_size, data_type=\"train\", shuffle=True):#\n        assert data_type in [\"train\", \"val\", \"test\"], \\\n            \"data_type must be 'train', 'val', or 'test'.\"\n\n        idx = self.indices[self.start_idx:self.start_idx + batch_size]\n\n        batch_x = self.X[idx]\n        batch_y = self.Y[idx] if self.Y is not None else self.Y\n        self.start_idx += batch_size\n\n        if self.start_idx + batch_size > self.num_examples:\n            self.reset_iteration()\n            self.epochs_completed += 1\n\n        return (batch_x, batch_y)\n    \n    def sample_random_batch(self, batch_size):\n        start_idx = np.random.randint(0, self.num_examples - batch_size)\n        batch_x = self.X[self.start_idx:self.start_idx + batch_size]\n        batch_y = self.Y[self.start_idx:self.start_idx + batch_size] if self.Y is not None else self.Y\n        \n        return (batch_x, batch_y)\n\n\ndef get_iterators(file, conv=False, datapoints=0):\n    data = np.load(file)\n    if conv:\n        img_shape = data[\"train_x\"][0,0].shape\n    else:\n        img_shape = data[\"train_x\"][0,0].flatten().shape\n    train_it = DataIterator(X=data[\"train_x\"].reshape(data[\"train_x\"].shape[:2]+img_shape)/255)\n    valid_it = DataIterator(X=data[\"valid_x\"].reshape(data[\"valid_x\"].shape[:2]+img_shape)/255)\n    test_it = DataIterator(X=data[\"test_x\"].reshape(data[\"test_x\"].shape[:2]+img_shape)/255)\n    return train_it, valid_it, test_it\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.877771Z","iopub.status.idle":"2024-04-14T14:32:53.878564Z","shell.execute_reply.started":"2024-04-14T14:32:53.878320Z","shell.execute_reply":"2024-04-14T14:32:53.878342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## runners/run_base.py","metadata":{}},{"cell_type":"code","source":"import os\nimport logging\nimport tensorflow as tf\n\n# tf.compat.v1.app.flags.DEFINE_integer(\"epochs\", 10, \"Epochs to train.\")\n# tf.compat.v1.app.flags.DEFINE_integer(\"batch_size\", 100, \"Training batch size\")\n# tf.compat.v1.app.flags.DEFINE_string(\"save_dir\", \"experiments/spring_color/\", \"Directory to save checkpoint and logs.\")\n# tf.compat.v1.app.flags.DEFINE_bool(\"use_ckpt\", False, \"Whether to start from scratch of start from checkpoint.\")\n# tf.compat.v1.app.flags.DEFINE_string(\"ckpt_dir\", \"\", \"Checkpoint dir to use.\")\n# tf.compat.v1.app.flags.DEFINE_float(\"base_lr\", 3e-4, \"Base learning rate.\")\n# tf.compat.v1.app.flags.DEFINE_bool(\"anneal_lr\", True, \"Whether to anneal lr after 0.75 of total epochs.\")\n# tf.compat.v1.app.flags.DEFINE_string(\"optimizer\", \"rmsprop\", \"Optimizer to use.\")\n# tf.compat.v1.app.flags.DEFINE_integer(\"save_every_n_epochs\", 5, \"Epochs between checkpoint saves.\")\n# tf.compat.v1.app.flags.DEFINE_integer(\"eval_every_n_epochs\", 1, \"Epochs between validation run.\")\n# tf.compat.v1.app.flags.DEFINE_integer(\"print_interval\", 10, \"Print train metrics every n mini-batches.\")\n# tf.compat.v1.app.flags.DEFINE_bool(\"debug\", False, \"If true, eval is not ran before training.\")\n# tf.compat.v1.app.flags.DEFINE_bool(\"test_mode\", False, \"If true, only run test set.\")\n\nlogger = logging.getLogger(\"tf\")\nlogger.setLevel(logging.DEBUG)\n# create console handler\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(message)s')\nch.setFormatter(formatter)\nlogger.addHandler(ch)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.879786Z","iopub.status.idle":"2024-04-14T14:32:53.880847Z","shell.execute_reply.started":"2024-04-14T14:32:53.880602Z","shell.execute_reply":"2024-04-14T14:32:53.880623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## runners/run_physics.py","metadata":{}},{"cell_type":"code","source":"# Configuration Constants\nEPOCHS = 10\nBATCH_SIZE = 100\nSAVE_DIR = \"experiments/spring_color/\"\nUSE_CKPT = False\nCKPT_DIR = \"\"\nBASE_LR = 3e-4\nANNEAL_LR = True\nOPTIMIZER = \"rmsprop\"\nSAVE_EVERY_N_EPOCHS = 5\nEVAL_EVERY_N_EPOCHS = 1\nPRINT_INTERVAL = 10\nDEBUG = False\nTEST_MODE = False\n\nTASK = \"spring_color\"\nMODEL = \"PhysicsNet\"\nRECURRENT_UNITS = 100\nLSTM_LAYERS = 1\nCELL_TYPE = \"\"\nENCODER_TYPE = \"conv_encoder\"\nDECODER_TYPE = \"conv_st_decoder\"\n\nAUTOENCODER_LOSS = 3.0\nALT_VEL = False\nCOLOR = True\nDATAPOINTS = 0","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.882408Z","iopub.status.idle":"2024-04-14T14:32:53.882837Z","shell.execute_reply.started":"2024-04-14T14:32:53.882631Z","shell.execute_reply":"2024-04-14T14:32:53.882647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport logging\nimport inspect\nimport tensorflow as tf\n# from nn.network import physics_models\n# from nn.utils.misc import classes_in_module\n# from nn.datasets.iterators import get_iterators\n# import runners.run_base","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.884754Z","iopub.status.idle":"2024-04-14T14:32:53.885174Z","shell.execute_reply.started":"2024-04-14T14:32:53.884968Z","shell.execute_reply":"2024-04-14T14:32:53.884984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.compat.v1.app.flags.DEFINE_string(\"task\", \"spring_color\", \"Type of task.\")\n# tf.compat.v1.app.flags.DEFINE_string(\"model\", \"PhysicsNet\", \"Model to use.\")\n# tf.compat.v1.app.flags.DEFINE_integer(\"recurrent_units\", 100, \"Number of units for each lstm, if using black-box dynamics.\")\n# tf.compat.v1.app.flags.DEFINE_integer(\"lstm_layers\", 1, \"Number of lstm cells to use, if using black-box dynamics\")\n# tf.compat.v1.app.flags.DEFINE_string(\"cell_type\", \"\", \"Type of pendulum to use.\")\n# tf.compat.v1.app.flags.DEFINE_string(\"encoder_type\", \"conv_encoder\", \"Type of encoder to use.\")\n# tf.compat.v1.app.flags.DEFINE_string(\"decoder_type\", \"conv_st_decoder\", \"Type of decoder to use.\")\n\n# tf.compat.v1.app.flags.DEFINE_float(\"autoencoder_loss\", 3.0, \"Autoencoder loss weighing.\")\n# tf.compat.v1.app.flags.DEFINE_bool(\"alt_vel\", False, \"Whether to use linear velocity computation.\")\n# tf.compat.v1.app.flags.DEFINE_bool(\"color\", True, \"Whether images are rbg or grayscale.\")\n# tf.compat.v1.app.flags.DEFINE_integer(\"datapoints\", 0, \"How many datapoints from the dataset to use. \\\n#                                               Useful for measuring data efficiency. default=0 uses all data.\")\n\n# FLAGS = tf.compat.v1.app.flags.FLAGS","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.887345Z","iopub.status.idle":"2024-04-14T14:32:53.887942Z","shell.execute_reply.started":"2024-04-14T14:32:53.887625Z","shell.execute_reply":"2024-04-14T14:32:53.887651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model = PhysicsNet\n\ndata_file, test_data_file, cell_type, seq_len, test_seq_len, input_steps, pred_steps, input_size = {\n    \"bouncing_balls\": (\n        \"bouncing/color_bounce_vx8_vy8_sl12_r2.npz\", \n        \"bouncing/color_bounce_vx8_vy8_sl30_r2.npz\", \n        \"bouncing_ode_cell\",\n        12, 30, 4, 6, 32*32),\n    \"spring_color\": (\n        \"spring_color/color_spring_vx8_vy8_sl12_r2_k4_e6.npz\", \n        \"spring_color/color_spring_vx8_vy8_sl30_r2_k4_e6.npz\",\n        \"spring_ode_cell\",\n        12, 30, 4, 6, 32*32),\n    \"spring_color_half\": (\n        \"spring_color_half/color_spring_vx4_vy4_sl12_r2_k4_e6_halfpane.npz\", \n        \"spring_color_half/color_spring_vx4_vy4_sl30_r2_k4_e6_halfpane.npz\", \n        \"spring_ode_cell\",\n        12, 30, 4, 6, 32*32),\n    \"3bp_color\": (\n        \"3bp_color/color_3bp_vx2_vy2_sl20_r2_g60_m1_dt05.npz\", \n        \"3bp_color/color_3bp_vx2_vy2_sl40_r2_g60_m1_dt05.npz\", \n        \"gravity_ode_cell\",\n        20, 40, 4, 12, 36*36),\n    \"mnist_spring_color\": (\n        \"mnist_spring_color/color_mnist_spring_vx8_vy8_sl12_r2_k2_e12.npz\", \n        \"mnist_spring_color/color_mnist_spring_vx8_vy8_sl30_r2_k2_e12.npz\", \n        \"spring_ode_cell\",\n        12, 30, 3, 7, 64*64)\n}[TASK]\n\ndef run_physics():\n    if not TEST_MODE:\n        network = Model(TASK, RECURRENT_UNITS, LSTM_LAYERS, cell_type, \n                        seq_len, input_steps, pred_steps,\n                       AUTOENCODER_LOSS, ALT_VEL, COLOR, \n                       input_size, ENCODER_TYPE, DECODER_TYPE)\n\n        network.build_graph()\n        network.build_optimizer(BASE_LR, OPTIMIZER, ANNEAL_LR)\n        network.initialize_graph(SAVE_DIR, USE_CKPT, CKPT_DIR)\n\n        data_iterators = get_iterators(\n                              os.path.join(\n                                  os.path.dirname(os.path.realpath(__file__)), \n                                  \"/kaggle/input/physics-as-inverse-graphics/%s\"%data_file), conv=True, datapoints=FLAGS.datapoints)\n        network.get_data(data_iterators)\n        network.train(EPOCHS, BATCH_SIZE, SAVE_EVERY_N_EPOCHS, EVAL_EVERY_N_EPOCHS,\n                    PRINT_INTERVAL, DEBUG)\n\n        tf.compat.v1.reset_default_graph()\n\n    network = Model(TASK, RECURRENT_UNITS, LSTM_LAYERS, cell_type, \n                    test_seq_len, input_steps, pred_steps,\n                   AUTOENCODER_LOSS, ALT_VEL, COLOR, \n                   input_size, ENCODER_TYPE, DECODER_TYPE)\n\n    network.build_graph()\n    network.build_optimizer(BASE_LR, OPTIMIZER, ANNEAL_LR)\n    network.initialize_graph(SAVE_DIR, True, CKPT_DIR)\n\n    data_iterators = get_iterators(\n                          os.path.join(\n                              os.path.dirname(os.path.realpath(__file__)), \n                              \"/kaggle/input/physics-as-inverse-graphics/%s\"%test_data_file), conv=True, datapoints=FLAGS.datapoints)\n    network.get_data(data_iterators)\n    network.train(0, BATCH_SIZE, SAVE_EVERY_N_EPOCHS, EVAL_EVERY_N_EPOCHS,\n                PRINT_INTERVAL, DEBUG)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:32:53.891185Z","iopub.status.idle":"2024-04-14T14:32:53.891822Z","shell.execute_reply.started":"2024-04-14T14:32:53.891585Z","shell.execute_reply":"2024-04-14T14:32:53.891611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.compat.v1.disable_eager_execution()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T10:53:10.792719Z","iopub.execute_input":"2024-04-09T10:53:10.793230Z","iopub.status.idle":"2024-04-09T10:53:10.800127Z","shell.execute_reply.started":"2024-04-09T10:53:10.793196Z","shell.execute_reply":"2024-04-09T10:53:10.798610Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}